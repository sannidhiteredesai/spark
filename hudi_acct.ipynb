{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"Hudi_Data_Processing_Framework\")\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "        .config(\"spark.sql.hive.convertMetastoreParquet\", \"false\")\n",
    "        .config(\"spark.jars.packages\",\n",
    "            \"org.apache.hudi:hudi-spark-bundle_2.11:0.8.0,org.apache.hudi:hudi-spark-bundle_2.11:0.5.3,org.apache.spark:spark-avro_2.11:2.4.5\")\n",
    "        .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = spark.createDataFrame(\n",
    "    [\n",
    "        (100, \"2015-01-01\", \"2015-01-01T13:51:39.340396Z\", 10),\n",
    "        (101, \"2015-01-01\", \"2015-01-01T12:14:58.597216Z\", 10),\n",
    "        (102, \"2015-01-01\", \"2015-01-01T13:51:40.417052Z\", 10),\n",
    "        (103, \"2015-01-01\", \"2015-01-01T13:51:40.519832Z\", 10),\n",
    "        (104, \"2015-01-02\", \"2015-01-01T12:15:00.512679Z\", 10),\n",
    "        (104, \"2015-01-02\", \"2015-01-01T12:15:00.512679Z\", 10),\n",
    "        (104, \"2015-01-02\", \"2015-01-02T12:15:00.512679Z\", 20),\n",
    "        (105, \"2015-01-02\", \"2015-01-01T13:51:42.248818Z\", 10),\n",
    "    ],\n",
    "    (\"acctid\", \"date\", \"ts\", \"deposit\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+-------+\n",
      "|acctid|      date|                  ts|deposit|\n",
      "+------+----------+--------------------+-------+\n",
      "|   100|2015-01-01|2015-01-01T13:51:...|     10|\n",
      "|   101|2015-01-01|2015-01-01T12:14:...|     10|\n",
      "|   102|2015-01-01|2015-01-01T13:51:...|     10|\n",
      "|   103|2015-01-01|2015-01-01T13:51:...|     10|\n",
      "|   104|2015-01-02|2015-01-01T12:15:...|     10|\n",
      "|   104|2015-01-02|2015-01-01T12:15:...|     10|\n",
      "|   104|2015-01-02|2015-01-02T12:15:...|     20|\n",
      "|   105|2015-01-02|2015-01-01T13:51:...|     10|\n",
      "+------+----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- acctid: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- ts: string (nullable = true)\n",
      " |-- deposit: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hudi_options = {\n",
    "    # ---------------DATA SOURCE WRITE CONFIGS---------------#\n",
    "    \"hoodie.table.name\": \"hudi_acct\",\n",
    "    \"hoodie.table.type\": \"MERGE_ON_READ\",\n",
    "    \"hoodie.datasource.write.operation\": \"upsert\",\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"acctid\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"ts\",\n",
    "    \"hoodie.datasource.write.partitionpath.field\": \"date\",\n",
    "    \"hoodie.datasource.write.hive_style_partitioning\": \"true\",\n",
    "    \"hoodie.upsert.shuffle.parallelism\": 8,\n",
    "    \"hoodie.insert.shuffle.parallelism\": 8,\n",
    "    \"hoodie.delete.shuffle.parallelism\": 8,\n",
    "#     \"hoodie.consistency.check.enabled\": True,\n",
    "#     \"hoodie.index.type\": \"BLOOM\",\n",
    "#     \"hoodie.index.bloom.num_entries\": 60000,\n",
    "#     \"hoodie.index.bloom.fpp\": 0.000000001,\n",
    "#     \"hoodie.cleaner.commits.retained\": 2,\n",
    "}\n",
    "\n",
    "hudi_dataset = 'F:\\PyCharm_Projects\\spark\\hudi\\op\\\\acct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hoodie.table.name': 'hudi_acct',\n",
       " 'hoodie.table.type': 'MERGE_ON_READ',\n",
       " 'hoodie.datasource.write.operation': 'upsert',\n",
       " 'hoodie.datasource.write.recordkey.field': 'acctid',\n",
       " 'hoodie.datasource.write.precombine.field': 'ts',\n",
       " 'hoodie.datasource.write.partitionpath.field': 'date',\n",
       " 'hoodie.datasource.write.hive_style_partitioning': 'true',\n",
       " 'hoodie.upsert.shuffle.parallelism': 8,\n",
       " 'hoodie.insert.shuffle.parallelism': 8,\n",
       " 'hoodie.delete.shuffle.parallelism': 8}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hudi_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT\n",
    "(\n",
    "    input_df.write.format(\"org.apache.hudi\")\n",
    "    .options(**hudi_options)\n",
    "    .mode(\"append\")\n",
    "    .save(hudi_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+----------+--------------------+-------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|acctid|      date|                  ts|deposit|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+----------+--------------------+-------+\n",
      "|     20210711181633| 20210711181633_0_16|               101|       date=2015-01-01|1cae8dd6-14ba-40a...|   101|2015-01-01|2015-01-01T12:14:...|     10|\n",
      "|     20210711181633| 20210711181633_0_17|               100|       date=2015-01-01|1cae8dd6-14ba-40a...|   100|2015-01-01|2015-01-01T13:51:...|     10|\n",
      "|     20210711181633| 20210711181633_0_18|               103|       date=2015-01-01|1cae8dd6-14ba-40a...|   103|2015-01-01|2015-01-01T13:51:...|     10|\n",
      "|     20210711181633| 20210711181633_0_19|               102|       date=2015-01-01|1cae8dd6-14ba-40a...|   102|2015-01-01|2015-01-01T13:51:...|     10|\n",
      "|     20210711181633| 20210711181633_1_20|               105|       date=2015-01-02|df2c13c1-ab34-40c...|   105|2015-01-02|2015-01-01T13:51:...|     10|\n",
      "|     20210711181633| 20210711181633_1_21|               104|       date=2015-01-02|df2c13c1-ab34-40c...|   104|2015-01-02|2015-01-02T12:15:...|     20|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# READ\n",
    "output_df = spark.read.format(\"org.apache.hudi\").load(hudi_dataset+\"/*/*\")\n",
    "output_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_df = spark.createDataFrame(\n",
    "    [\n",
    "        (100, \"2015-01-01\", \"2015-01-01T13:51:39.340396Z\", 20),\n",
    "#         (201, \"2015-01-01\", \"2015-01-01T12:14:58.597216Z\", 10),\n",
    "#         (202, \"2015-01-01\", \"2015-01-01T13:51:40.417052Z\", 10),\n",
    "#         (203, \"2015-01-01\", \"2015-01-01T13:51:40.519832Z\", 10),\n",
    "#         (204, \"2015-01-02\", \"2015-01-01T12:15:00.512679Z\", 10),\n",
    "#         (204, \"2015-01-03\", \"2015-01-01T12:15:00.512679Z\", 10),\n",
    "#         (204, \"2015-01-04\", \"2015-01-02T12:15:00.512679Z\", 20),\n",
    "#         (205, \"2015-01-02\", \"2015-01-01T13:51:42.248818Z\", 10),\n",
    "    ],\n",
    "    (\"acctid\", \"date\", \"ts\", \"deposit\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+-------+\n",
      "|acctid|      date|                  ts|deposit|\n",
      "+------+----------+--------------------+-------+\n",
      "|   100|2015-01-01|2015-01-01T13:51:...|     20|\n",
      "+------+----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "update_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE\n",
    "(\n",
    "    update_df.write.format(\"org.apache.hudi\")\n",
    "    .options(**hudi_options)\n",
    "    .mode(\"append\")\n",
    "    .save(hudi_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+----------+--------------------+-------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|acctid|      date|                  ts|deposit|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+----------+--------------------+-------+\n",
      "|     20210710182316|  20210710182316_1_1|               105|       date=2015-01-02|47b34019-f7d2-479...|   105|2015-01-02|2015-01-01T13:51:...|     10|\n",
      "|     20210710182316|  20210710182316_1_3|               104|       date=2015-01-02|47b34019-f7d2-479...|   104|2015-01-02|2015-01-02T12:15:...|     20|\n",
      "|     20210710183022|  20210710183022_0_7|               100|       date=2015-01-01|eda4dbef-2e4c-4fe...|   100|2015-01-01|2015-01-01T13:51:...|     20|\n",
      "|     20210710182316|  20210710182316_0_5|               103|       date=2015-01-01|eda4dbef-2e4c-4fe...|   103|2015-01-01|2015-01-01T13:51:...|     10|\n",
      "|     20210710182316|  20210710182316_0_6|               102|       date=2015-01-01|eda4dbef-2e4c-4fe...|   102|2015-01-01|2015-01-01T13:51:...|     10|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# READ\n",
    "output_df = spark.read.format(\"org.apache.hudi\").load(hudi_dataset+\"/*/*\")\n",
    "output_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+----------+--------------------+-------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|acctid|      date|                  ts|deposit|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+----------+--------------------+-------+\n",
      "|     20210710183022|  20210710183022_0_7|               100|       date=2015-01-01|eda4dbef-2e4c-4fe...|   100|2015-01-01|2015-01-01T13:51:...|     20|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "incremental_read_options = {\n",
    "  'hoodie.datasource.query.type': 'incremental',\n",
    "  'hoodie.datasource.read.begin.instanttime': '20210710182316', # Will return commits after this value\n",
    "}\n",
    "# READ\n",
    "output_df = spark.read.format(\"org.apache.hudi\").options(**incremental_read_options).load(hudi_dataset)\n",
    "output_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hoodie.table.name': 'hudi_acct',\n",
       " 'hoodie.table.type': 'MERGE_ON_READ',\n",
       " 'hoodie.datasource.write.operation': 'delete',\n",
       " 'hoodie.datasource.write.recordkey.field': 'acctid',\n",
       " 'hoodie.datasource.write.precombine.field': 'ts',\n",
       " 'hoodie.datasource.write.partitionpath.field': 'date',\n",
       " 'hoodie.datasource.write.hive_style_partitioning': 'true',\n",
       " 'hoodie.upsert.shuffle.parallelism': 8,\n",
       " 'hoodie.insert.shuffle.parallelism': 8,\n",
       " 'hoodie.delete.shuffle.parallelism': 8}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hudi_delete_options = hudi_options.copy()\n",
    "hudi_delete_options['hoodie.datasource.write.operation'] = 'delete'\n",
    "\n",
    "hudi_delete_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|acctid|      date|\n",
      "+------+----------+\n",
      "|   102|2015-01-01|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# key and partition both are compulsory for delete to work\n",
    "delete_df = spark.createDataFrame( [(102, \"2015-01-01\")], (\"acctid\", \"date\") ) \n",
    "delete_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "(\n",
    "    delete_df.write.format(\"org.apache.hudi\")\n",
    "    .options(**hudi_delete_options)\n",
    "    .mode(\"append\")\n",
    "    .save(hudi_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+----------+--------------------+-------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|acctid|      date|                  ts|deposit|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+----------+--------------------+-------+\n",
      "|     20210710182316|  20210710182316_0_5|               103|       date=2015-01-01|eda4dbef-2e4c-4fe...|   103|2015-01-01|2015-01-01T13:51:...|     10|\n",
      "|     20210710182316|  20210710182316_1_1|               105|       date=2015-01-02|47b34019-f7d2-479...|   105|2015-01-02|2015-01-01T13:51:...|     10|\n",
      "|     20210710182316|  20210710182316_1_3|               104|       date=2015-01-02|47b34019-f7d2-479...|   104|2015-01-02|2015-01-02T12:15:...|     20|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------+----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# READ\n",
    "output_df = spark.read.format(\"org.apache.hudi\").load(hudi_dataset+\"/*/*\")\n",
    "output_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
